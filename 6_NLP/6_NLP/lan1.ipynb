{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "de 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2+3}{} {2*4-6}{} {2+8-1}{} {6+7-8-9*5}{}\n",
      "None\n",
      "This is {a }ball. This is {the }tree. Lets eat {an }apple\n",
      "None\n",
      "Match: 2+3\n",
      "Match: 2*4-6\n",
      "Match: 2+8-1\n",
      "Match: 6+7-8-9*5\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Write regular expressions to match the following classes of strings:\n",
    "1. An arithmetic expression using integers, addition, and multiplication, such as 5*6+9\n",
    "2. A single determiner (assume that a, an, and the are the only determiners).\n",
    "'''\n",
    "import nltk\n",
    "\n",
    "\n",
    "test_exp=\"2+3 2*4-6 2+8-1 6+7-8-9*5\"\n",
    "print(nltk.re_show(r'([0-9][\\+\\-\\*]|[0-9])*',test_exp))\n",
    "test_string = \"This is a ball. This is the tree. Lets eat an apple\"\n",
    "print(nltk.re_show(r'(a\\s|an\\s|the\\s)',test_string))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "# Arithmetic expression pattern\n",
    "arithmetic_pattern = re.compile(r'([0-9][\\+\\-\\*]|[0-9])*')\n",
    "\n",
    "# Test case string\n",
    "test_string = \"2+3 2*4-6 2+8-1 6+7-8-9*5\"\n",
    "\n",
    "# Split the string into individual expressions\n",
    "expressions = test_string.split()\n",
    "\n",
    "# Test each expression\n",
    "for expr in expressions:\n",
    "    if arithmetic_pattern.fullmatch(expr):\n",
    "        print(f'Match: {expr}')\n",
    "    else:\n",
    "        print(f'No match: {expr}')\n",
    "\n",
    "# tu lam voi pattern kia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a sample sentence.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Write code that removes whitespace at the beginning and end of a string, and normalizes whitespace between words to be a single space character.\n",
    "Input:\n",
    "s = '    this is. \\n a sample \\t. sentence.     '\n",
    "Output: \n",
    "'this is a sample sentence.\"\n",
    "'''\n",
    "\n",
    "#c1\n",
    "s = ' this is \\n a sample\\t sentence. '\n",
    "' '.join(s.split())\n",
    "\n",
    "#c2\n",
    "import re\n",
    "s_ = re.sub(r'^\\s|\\s$', '', s)  \n",
    "re.sub(r'\\s+', ' ', s_)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write  function shorten   to process   omitting   most frequently occurring words of   How readable is it'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Write a function ** shorten(text, n) ** to process a text, omitting the ** n ** most frequently occurring words of the text. How readable is it?\n",
    "'''\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "\n",
    "def shorten(text, n):\n",
    "    splits = re.findall(r\"\\w+(?:[-']\\w+)*\", text)\n",
    "    fdist = nltk.FreqDist(splits)\n",
    "    # most_freq = fdist.most_common(n)\n",
    "    # print(most_freq)\n",
    "    most_freq = []\n",
    "    for sample in fdist.most_common(n):\n",
    "        most_freq.append(sample[0])\n",
    "    for i in range(len(splits)):\n",
    "        if splits[i] in most_freq:\n",
    "            splits[i] = ''                     # in fact, the element should be deleted in the list\n",
    "    return ' '.join(splits)\n",
    "\n",
    "text = 'Write a function shorten(text, n) to process a text, omitting the n most frequently occurring words of the text. How readable is it?'\n",
    "shorten(text, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Walter', 'feeling anxious', 'He', 'diagnosed today', 'He probably', 'best person I know']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Tokenize the given text with stop words (“is”,”the”,”was”) as delimiters. Tokenizing this way identifies meaningful phrases. Sometimes, useful for topic modeling\n",
    "\n",
    "Input :\n",
    "text = \"Walter was feeling anxious. He was diagnosed today. He probably is the best person I know.\"\"\n",
    "Expected Output :\n",
    "\n",
    "['Walter', 'feeling anxious', 'He', 'diagnosed today', 'He probably', 'best person I know']\n",
    "'''\n",
    "text = \"Walter was feeling anxious. He was diagnosed today. He probably is the best person I know.\"\n",
    "stopwords_and_delims = ['was', 'is', 'the', '.', ',', '-', '!', '?']\n",
    "for r in stopwords_and_delims:\n",
    "    text = text.replace(r, 'DELIM')\n",
    "words = [t.strip() for t in text.split('DELIM')]\n",
    "words_filtered = list(filter(lambda a: a not in [''], words))\n",
    "print(words_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is . a sample . sentence.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def normalize_whitespace_nltk(input_string):\n",
    "    # Tokenize the input string into words\n",
    "    words = word_tokenize(input_string)\n",
    "\n",
    "    # Detokenize the words to normalize whitespace\n",
    "    normalized_string = TreebankWordDetokenizer().detokenize(words)\n",
    "\n",
    "    return normalized_string\n",
    "\n",
    "# Input string\n",
    "s = '    this is. \\n a sample \\t. sentence.     '\n",
    "\n",
    "# Normalize whitespace using NLTK\n",
    "output = normalize_whitespace_nltk(s)\n",
    "\n",
    "# Output\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "de 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match: back\n",
      "Match: the the the\n",
      "Match: blue\n",
      "No match: the bug bug\n",
      "No match: Ula Ola Ole\n",
      "No match: Banana\n",
      "No match: the big bug\n",
      "Match: tu tu tu test test test\n",
      "Match: back\n",
      "Match: blue\n",
      "No match: Banana\n",
      "Match: bat\n",
      "No match: Bee\n",
      "No match: Ula Ola Ole\n",
      "Match: the the the\n",
      "No match: the bug bug\n",
      "No match: the big bug\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Write regular expressions to match the following classes of strings:\n",
    "1. The set of all lowercase alphabetic strings starting with a b, such as back \n",
    "2. the set of all strings with three consecutive repeated wons (eg. \"Ula Ola Ole\" and \"the the the\" but not \"the bug bug\" or \"the big bug\").\n",
    "'''\n",
    "\n",
    "import re\n",
    "\n",
    "# Merged pattern for both cases\n",
    "merged_pattern = re.compile(r'^b[a-z]*$|\\b(\\w+)\\s+\\1\\s+\\1\\b')\n",
    "\n",
    "# Test cases\n",
    "test_strings = [\n",
    "    \"back\",          # Should match case 1\n",
    "    \"the the the\",   # Should match case 2\n",
    "    \"blue\",          # Should not match any case\n",
    "    \"the bug bug\",   # Should not match any case\n",
    "    \"Ula Ola Ole\",   # Should match case 2\n",
    "    \"Banana\",        # Should not match any case\n",
    "    \"the big bug\",   # Should not match any case\n",
    "    \"tu tu tu test test test\"\n",
    "]\n",
    "\n",
    "for s in test_strings:\n",
    "    if merged_pattern.search(s):\n",
    "        print(f'Match: {s}')\n",
    "    else:\n",
    "        print(f'No match: {s}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "# 1. Matching lowercase alphabetic strings starting with a \"b\"\n",
    "pattern1 = re.compile(r'^b[a-z]*$')\n",
    "\n",
    "# Test cases\n",
    "strings1 = [\"back\", \"blue\", \"Banana\", \"bat\", \"Bee\"]\n",
    "for s in strings1:\n",
    "    if pattern1.match(s):\n",
    "        print(f'Match: {s}')\n",
    "    else:\n",
    "        print(f'No match: {s}')\n",
    "\n",
    "# 2. Matching strings with three consecutive repeated words\n",
    "pattern2 = re.compile(r'\\b(\\w+)\\s+\\1\\s+\\1\\b')\n",
    "\n",
    "# Test cases\n",
    "strings2 = [\"Ula Ola Ole\", \"the the the\", \"the bug bug\", \"the big bug\"]\n",
    "for s in strings2:\n",
    "    if pattern2.search(s):\n",
    "        print(f'Match: {s}')\n",
    "    else:\n",
    "        print(f'No match: {s}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The match has concluded Vietnam has won the match Will we win the finals too\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Write code that removes all the punctuations in the given text\n",
    "Input:\n",
    " text = \"The match has concluded !!! Vietnam has won the match. Will we win the finals too ? !!\"\n",
    "Output:\n",
    "\"The match has concluded Vietnam has won the match Will we win the final too\"\n",
    "'''\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove punctuation from the list of words\n",
    "    words_without_punctuation = [word for word in words if word not in punctuation]\n",
    "\n",
    "    # Join the words back into a sentence\n",
    "    result_text = ' '.join(words_without_punctuation)\n",
    "\n",
    "    return result_text\n",
    "\n",
    "# Input text\n",
    "text = \"The match has concluded !!! Vietnam has won the match. Will we win the finals too ? !!\"\n",
    "\n",
    "# Remove punctuation\n",
    "result = remove_punctuation(text)\n",
    "\n",
    "# Output\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stge', 'woan', 'lyng', 'pods', 'ding', 'swds', 'bais', 'syem', 'gont']\n",
      "['Stge', 'woan', 'lyng', 'pods', 'ding', 'swds', 'bais', 'syem', 'gont']\n",
      "['Stge', 'woen', 'lyng', 'pods', 'ding', 'swds', 'bais', 'syem', 'gont']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Write a function that takes a list of words and concatenates the first two and the last two characters of each word that has at least 4 characters.\n",
    "Input:\n",
    "['Strange', 'woman', \"lying', '', 'ponds', 'distributing', 'swords', \"is'', 'no', 'basis', 'system', 'for', 'a', 'system', 'of', 'government']\n",
    "Output:\n",
    "[\"Stge\", \"women\", \"lyng\", \"pods\", \"ding\", \"swds\", \"basis\", \"syem\", \"gont\"]\n",
    "'''\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def manipulate_words(word_list):\n",
    "    manipulated_words = []\n",
    "\n",
    "    for word in word_list:\n",
    "        # Check if the word has at least 4 characters\n",
    "        if len(word) >= 4:\n",
    "            # Concatenate the first two and last two characters of the word\n",
    "            manipulated_word = word[:2] + word[-2:]\n",
    "            manipulated_words.append(manipulated_word)\n",
    "\n",
    "    return manipulated_words\n",
    "\n",
    "# Input list of words\n",
    "word_list = ['Strange', 'woman', 'lying', '', 'ponds', 'distributing', 'swords', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government']\n",
    "\n",
    "# Manipulate words\n",
    "result = manipulate_words(word_list)\n",
    "\n",
    "# Output\n",
    "print(result)\n",
    "\n",
    "\n",
    "\n",
    "def q3(list_of_words):\n",
    "    return [ word[:2:] + word[-2::] for word in list_of_words if len(word) >= 4]\n",
    "\n",
    "print(q3(['Strange', 'woman', 'lying', '', 'ponds', 'distributing', 'swords', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government']))\n",
    "\n",
    "\n",
    "\n",
    "def Q3(text):\n",
    "    text_4char = [w for w in text if len(w) >=4 and w.isalpha()]\n",
    "    result = []\n",
    "    for word in text_4char:\n",
    "        result_string = word[:2] + word[-2:]\n",
    "        result.append(result_string)\n",
    "    return result\n",
    "\n",
    "text = ['Strange', 'women', 'lying', 'in', 'ponds', 'distributing', 'swords','is','no','basis','for','a','system','of','gorvenment','.']\n",
    "print(Q3(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Strange', 'women', 'lying', 'in', 'ponds', 'distributing', 'swords', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.']\n",
      "Stralangele wolomelen lylyiling ilin polonds dilistrilibulutiling swolords ilis nolo balasilis folor ala sylystelem olof golovelernmelent .\n",
      "Stralangele wolomelen lylyiling ilin polonds dilistrilibulutiling swolords ilis nolo balasilis folor ala sylystelem olof golovelernmelent.\n",
      "Stralangele wolomelen lylyiling ilin polonds dilistrilibulutiling swolords ilis nolo balasilis folor ala sylystelem olof golovelernmelent.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Write a function that takes a string and doubles each vowel (aciouy), inserting the \"l\" in between.\n",
    "Input:\n",
    "text = \"Strange women lying in ponds distributing swords is no basis for a system of government.\"\n",
    "Output:\n",
    "\"Stralangele wolomelen lylyiling ilin polonds dilistrilibulutiling swolords ilis nolo balasilis folor ala sylystelem olof golovelernmelent.\"\n",
    "'''\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def double_vowels_with_l(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    print(words)\n",
    "\n",
    "    # Define the set of vowels\n",
    "    vowels = set('aeiouy')\n",
    "\n",
    "    # Function to double vowels with 'l' in between\n",
    "    def double_vowels(word):\n",
    "        return ''.join([f'{char}l{char}' if char.lower() in vowels else char for char in word])\n",
    "\n",
    "    # Double vowels in each word\n",
    "    doubled_words = [double_vowels(word) for word in words]\n",
    "\n",
    "    # Join the words back into a sentence\n",
    "    result_text = ' '.join(doubled_words)\n",
    "\n",
    "    return result_text\n",
    "\n",
    "# Input text\n",
    "text = \"Strange women lying in ponds distributing swords is no basis for a system of government.\"\n",
    "\n",
    "# Double vowels with 'l'\n",
    "result = double_vowels_with_l(text)\n",
    "\n",
    "# Output\n",
    "print(result)\n",
    "\n",
    "\n",
    "def q4(string):\n",
    "    list_of_string = string.split()\n",
    "    result_list = []\n",
    "    for substring in list_of_string:\n",
    "        temp = ''\n",
    "        for char in substring:\n",
    "            if char in 'aeiouy':\n",
    "                temp += f'{char}l{char}'\n",
    "            else:\n",
    "                temp += f'{char}'\n",
    "        result_list.append(temp)\n",
    "    \n",
    "    return ' '.join(result_list)\n",
    "\n",
    "print(q4(\"Strange women lying in ponds distributing swords is no basis for a system of government.\"))\n",
    "\n",
    "print(\"Stralangele wolomelen lylyiling ilin polonds dilistrilibulutiling swolords ilis nolo balasilis folor ala sylystelem olof golovelernmelent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
